---
layout: post
title: 每天10分钟系列之 --- 了解Hive
image: /img/hello_world.jpeg
date:  2016-05-26 19:00:00 +0800  
description: 开发工具
img: post-9.jpg # Add image post (optional)
tags: [arc]
labels: [hive, 每天10分钟]
author: # Add name author (optional)
arc: true
---

### 基本概念
- Hive是基于Hadoop，是Facebook为了解决海里日志数据的分析而开发了Hive. Hive是HDFS之上的数据仓库基础架构，`本质上是一个数据库，但有别于通常意义的数据库`，是一种用类SQL语句来协助读写，管理那些存储在分布式存储系统上大数据集的数据仓库软件。而传统的Orace 或 MySql数据仓库是直接建立再Oracle或MySql数据库之上的。
    - Hive 可以用来进行数据提取，转化，加载（ETL).
    - Hive定义了简单的类似SQL查询语言，称为HQL，它允许熟悉SQL的用户查询数据。
    - Hive允许熟悉MapReduce开发者的开发自定义的mapper和reducer来处理内建mapper和reduce无法完成的复杂的分析工作。
    - Hive是SQL解析引擎，它将SQL语句转化为Map, REduce Job 然后在Hadoop执行
    - Hive 的表其实就是存储在HDFS的目录/文件，Hive本身并不提供数据的存储功能。
    - Hive是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数控上。
    

![Hive结构](http://p6jsga0vv.bkt.clouddn.com/18-11-7/78607355.jpg)

### Hive的体系结构

#### 元数据
- Hive的元数据存储在数据库中（metastore)，支持mysql，derby,oracle等传统数据库。
- Hive中元数据包括表的名字，表的列和分区及其属性，表的数据所在目录等等。
- 元数据被默认创建在derby数据库中，以表的形式保存数据

![hive.jpg](http://p6jsga0vv.bkt.clouddn.com/18-11-7/73083763.jpg)

####  Hive之HQL的执行过程
  Hive的执行器，是将最终要执行的MapReduce程序放到YARN上一系列Job的方式去执行的。
  
- 解析器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成，生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行。
- 过程： HQL语句 ---》 解析器：词法分析---》编译器：生成HQL的执行计划---》优化器：生成最优执行计划---》执行。

#### Hive的底层存储
- Hive的数据是存储在HDFS上的，Hive中的库和表可以看作是对HDFS上数据做的一个映射。所以Hive必须是运行在一个Hadoop集群上的。

#### Hive的元数据存储
- Hive的元数据一般是存储在MySQL上这种关系型数据库的，Hive和MySQl之间通过MetaStore服务交互。


| 元数据项 | 说明 | 
| :------ |:--- | :--- |
| Owner | 库，表的所有者 | 
| LastAccessTime | 最后修改时间 | 
| Table Type | 表类型（内部表，外部表） | 
| Create Time | 创建时间 |
| Location | 存储位置 |

#### Hive 客户端
- hive有很多客户端：
    - cli命令客户端：采用交互窗口，用Hive命令和Hive进行通信。
    - HiveServer2客户端：用Thrift协议进行通信，Thrift是不同语言之间的转换器，是连接不同语言程序间的协议，通过JDBC或者ODBC去访问Hive。
    - HWI客户端：hive自带的一个客户端，比较粗糙，一般不用。
    - HUE客户端：通过Web页面来和Hive进行交互，使用的比较多。

#### MapReduce执行过程简介
- MapReduce过程大体分为两个阶段：map函数阶段和reduce函数阶段，两个阶段之间有个shuffle。
    - Hadoop将MapReduce输入的数据划分为等长的小分片，一般每个分片是128M, 因为HDFS的每个块是128M。Hadoop1.X中这个数是64M。
    - `Map函数是数据准备阶段`，读取分片内容，并筛选掉不需要的数据，将数据解析为键值对的形式输出，map函数核心目的是形成对数据的索引，以供reduce函数方便对数据进行分析。
    - 在map函数执行完后，进行map端的shuffle过程，map， map端的shuffle是将map函数的输出进行区分，不同分区的数据要传入不同的Reduce里去。
    - 各个分区里的数据传入Reduce后，会先进行Reduce端的Shuffle过程，这里会将各个Map传递过来的相同分区进行排序，然后进行分组，一个分组的数据执行一次reduce函数。
    - reduce函数以分组的数据为数据源，对数据进行相应的分析，输出的结果为最后的目标数据。
    - 由于map任务的输出结果传递给reduce任务过程中，是在节点间的传输，是占用带宽的，这样带宽就制约了程序执行过程的最大吞吐量，为了减少map和reduce间数据传输，在map后面添加了combiner函数来就map结果进行预处理，combiner函数是运行在map所在节点的。

- 工作机制
 
![](http://p6jsga0vv.bkt.clouddn.com/18-11-7/48196677.jpg)

![](http://p6jsga0vv.bkt.clouddn.com/18-11-7/76648895.jpg)


### 总结

- 还是那句话，Hive入门使用很容易，这得益于它采用了类似与SQL语句的方式与用户交互，这也是Hive被大量使用的原因，但是最好还是理解Hive背后的执行原理，这样才能开发出高效的程序。


### 参考资料
 
[hive官方文档](https://cwiki.apache.org/confluence/display/Hive/GettingStarted.)

{: .box-note}
**Note:** 本文非作为商业用途，侵删. 如果你遇到任何问题，欢迎下面留言哈！ 整理不易，欢迎打赏！
